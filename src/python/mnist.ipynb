{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "batch_size = 100\n",
    "data_path='../data/mnist'\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))\n",
    "            ])\n",
    "# transform = transforms.Compose([\n",
    "#             transforms.Resize((28, 28)),\n",
    "#             transforms.ToTensor(),\n",
    "#             ])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "num_inputs = 28*28\n",
    "num_hidden = 256\n",
    "num_outputs = 10\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 20\n",
    "beta = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        spike_input = spikegen.rate(x, num_steps=num_steps) # Generate spike trains\n",
    "        # print(\"spike_input\")\n",
    "        # print(spike_input.shape)\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            # print(spike_input[step].sum(axis=-1))\n",
    "            # print(spike_input[step].shape)\n",
    "            cur1 = self.fc1(spike_input[step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            # print(\"spk1\")\n",
    "            # print(spk1.shape)\n",
    "            # print(spk1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            # print(cur1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            # print(\"spk2\")\n",
    "            # print(spk2.shape)\n",
    "            # print(spk2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0\n",
      "Train Set Loss: 47.33\n",
      "Test Set Loss: 46.41\n",
      "Train set accuracy for a single minibatch: 23.00%\n",
      "Test set accuracy for a single minibatch: 16.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 50\n",
      "Train Set Loss: 14.90\n",
      "Test Set Loss: 11.93\n",
      "Train set accuracy for a single minibatch: 84.00%\n",
      "Test set accuracy for a single minibatch: 90.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 100\n",
      "Train Set Loss: 8.71\n",
      "Test Set Loss: 10.67\n",
      "Train set accuracy for a single minibatch: 91.00%\n",
      "Test set accuracy for a single minibatch: 94.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 150\n",
      "Train Set Loss: 7.47\n",
      "Test Set Loss: 5.93\n",
      "Train set accuracy for a single minibatch: 92.00%\n",
      "Test set accuracy for a single minibatch: 94.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 200\n",
      "Train Set Loss: 10.94\n",
      "Test Set Loss: 9.19\n",
      "Train set accuracy for a single minibatch: 91.00%\n",
      "Test set accuracy for a single minibatch: 87.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 250\n",
      "Train Set Loss: 6.14\n",
      "Test Set Loss: 10.06\n",
      "Train set accuracy for a single minibatch: 95.00%\n",
      "Test set accuracy for a single minibatch: 89.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 300\n",
      "Train Set Loss: 6.44\n",
      "Test Set Loss: 7.86\n",
      "Train set accuracy for a single minibatch: 94.00%\n",
      "Test set accuracy for a single minibatch: 88.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 350\n",
      "Train Set Loss: 6.99\n",
      "Test Set Loss: 5.32\n",
      "Train set accuracy for a single minibatch: 93.00%\n",
      "Test set accuracy for a single minibatch: 91.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 400\n",
      "Train Set Loss: 9.19\n",
      "Test Set Loss: 6.89\n",
      "Train set accuracy for a single minibatch: 94.00%\n",
      "Test set accuracy for a single minibatch: 92.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 450\n",
      "Train Set Loss: 4.43\n",
      "Test Set Loss: 6.48\n",
      "Train set accuracy for a single minibatch: 94.00%\n",
      "Test set accuracy for a single minibatch: 94.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 500\n",
      "Train Set Loss: 4.19\n",
      "Test Set Loss: 7.60\n",
      "Train set accuracy for a single minibatch: 96.00%\n",
      "Test set accuracy for a single minibatch: 89.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 550\n",
      "Train Set Loss: 9.09\n",
      "Test Set Loss: 3.94\n",
      "Train set accuracy for a single minibatch: 94.00%\n",
      "Test set accuracy for a single minibatch: 95.00%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output, _ = net(data.view(batch_size, -1))\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, train=False)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "        # print(mem_rec.shape)\n",
    "        # print(mem_rec)\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        for step in range(num_steps):\n",
    "            loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "            for step in range(num_steps):\n",
    "                test_loss += loss(test_mem[step], test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            if counter % 50 == 0:\n",
    "                train_printer()\n",
    "            counter += 1\n",
    "            iter_counter +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correctly classified test set images: 9369/10000\n",
      "Test Set Accuracy: 93.69%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# drop_last switched to False to keep all samples\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "  net.eval()\n",
    "  for data, targets in test_loader:\n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    test_spk, _ = net(data.view(data.size(0), -1))\n",
    "\n",
    "    # calculate total accuracy\n",
    "    _, predicted = test_spk.sum(dim=0).max(1)\n",
    "    total += targets.size(0)\n",
    "    correct += (predicted == targets).sum().item()\n",
    "print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
    "print(f\"Test Set Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for CSV output.\n",
    "spike_save_path = \"mnist_input_spikes.csv\"\n",
    "label_save_path = \"mnist_labels.csv\"\n",
    "\n",
    "all_spikes = []\n",
    "all_labels = []\n",
    "\n",
    "# Loop over your test_loader.\n",
    "for data, targets in test_loader:\n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    # Convert images to spike trains.\n",
    "    # Assume spike_data has shape (num_steps, batch_size, vector_length)\n",
    "    spike_data = spikegen.rate(data.view(batch_size, -1), num_steps=num_steps).cpu().numpy()\n",
    "    # Remove the batch dimension (assumed to be 1)\n",
    "    spike_data = np.squeeze(spike_data, axis=1)  # Now shape is (num_steps, vector_length)\n",
    "    all_spikes.append(spike_data)\n",
    "    \n",
    "    # For labels, assume each batch yields one label.\n",
    "    all_labels.append(targets.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches along the time dimension.\n",
    "all_spikes = np.concatenate(all_spikes, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Save spike data and labels as CSV.\n",
    "# The CSV file for spikes will have (total_time_steps x vector_length) entries.\n",
    "np.savetxt(spike_save_path, all_spikes.astype(np.int8), delimiter=\",\", fmt=\"%d\")\n",
    "np.savetxt(label_save_path, all_labels.astype(np.int8), delimiter=\",\", fmt=\"%d\")\n",
    "\n",
    "print(\"Spike data and labels saved as CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"weights_fc1.txt\", net.fc1.weight.detach().numpy())\n",
    "np.savetxt(\"weights_fc2.txt\", net.fc2.weight.detach().numpy())\n",
    "np.savetxt(\"bias_fc1.txt\", net.fc1.bias.detach().numpy())\n",
    "np.savetxt(\"bias_fc2.txt\", net.fc2.bias.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, label in test_loader:\n",
    "    break\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d =  data[0,0,:,:] * 255\n",
    "f = d.cpu().numpy()\n",
    "np.savetxt(\"input_image.txt\", f.flatten())\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# get the weights and bias of every layer in the network and put it in one array\n",
    "w = net.fc1.weight.detach().cpu().numpy()\n",
    "b = net.fc1.bias.detach().cpu().numpy()\n",
    "w = np.concatenate((w.flatten(), b.flatten()), axis=0)\n",
    "w2 = net.fc2.weight.detach().cpu().numpy()\n",
    "b2 = net.fc2.bias.detach().cpu().numpy()\n",
    "w2 = np.concatenate((w2.flatten(), b2.flatten()), axis=0)\n",
    "a = np.concatenate((w, w2), axis=0)\n",
    "\n",
    "# find absolute max and min of the weights and bias\n",
    "max_val = np.max(a)\n",
    "min_val = np.min(a)\n",
    "print(\"Max weight: \", max_val)\n",
    "print(\"Min weight: \", min_val)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Weight Distribution of fc1 Layer\")\n",
    "plt.xlabel(\"Weight Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(False)\n",
    "\n",
    "plt.hist(a.flatten(), bins=256, range=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q07_SCALE = 128.0\n",
    "Q07_MAX_FLOAT = 127 / 128.0    # 0.9921875\n",
    "Q07_MIN_FLOAT = -1.0\n",
    "Q07_MAX_INT8 = 127\n",
    "Q07_MIN_INT8 = -128\n",
    "\n",
    "def quantize_q07(x: float) -> int:\n",
    "    if x > Q07_MAX_FLOAT:\n",
    "        x = Q07_MAX_FLOAT\n",
    "    elif x < Q07_MIN_FLOAT:\n",
    "        x = Q07_MIN_FLOAT\n",
    "\n",
    "    scaled = int(x * Q07_SCALE + (0.5 if x >= 0 else -0.5))\n",
    "\n",
    "    if scaled > Q07_MAX_INT8:\n",
    "        scaled = Q07_MAX_INT8\n",
    "    elif scaled < Q07_MIN_INT8:\n",
    "        scaled = Q07_MIN_INT8\n",
    "\n",
    "    return np.int8(scaled)\n",
    "\n",
    "def quantize_tensor_q07(tensor: torch.Tensor):\n",
    "    tensor_np = tensor.detach().cpu().numpy()\n",
    "    quantized = np.vectorize(quantize_q07)(tensor_np).astype(np.int8)\n",
    "    return quantized\n",
    "\n",
    "def format_c_array(var_name: str, array: np.ndarray) -> str:\n",
    "    shape = array.shape\n",
    "    dims = ''.join([f\"[{d}]\" for d in shape])\n",
    "    result = f\"const int8_t {var_name}{dims} = {{\\n\"\n",
    "\n",
    "    if array.ndim == 1:\n",
    "        result += \"    { \" + ', '.join(str(v) for v in array) + \" }\\n\"\n",
    "    elif array.ndim == 2:\n",
    "        for row in array:\n",
    "            result += \"    { \" + ', '.join(str(v) for v in row) + \" },\\n\"\n",
    "    elif array.ndim == 3:\n",
    "        for mat in array:\n",
    "            result += \"    {\\n\"\n",
    "            for row in mat:\n",
    "                result += \"        { \" + ', '.join(str(v) for v in row) + \" },\\n\"\n",
    "            result += \"    },\\n\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported tensor dimension: {array.ndim}\")\n",
    "\n",
    "    result += \"};\\n\\n\"\n",
    "    return result\n",
    "\n",
    "def write_c_header(model: torch.nn.Module, header_path: str):\n",
    "    with open(header_path, 'w') as f:\n",
    "        f.write(\"// Auto-generated Q0.7 quantized weights\\n\\n\")\n",
    "        f.write(\"#ifndef Q07_WEIGHTS_H\\n#define Q07_WEIGHTS_H\\n\\n\")\n",
    "        f.write(\"#include <stdint.h>\\n\\n\")\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            q_data = quantize_tensor_q07(param)\n",
    "            c_name = name.replace('.', '_')\n",
    "            f.write(f\"// Shape: {q_data.shape}\\n\")\n",
    "            f.write(format_c_array(c_name, q_data))\n",
    "\n",
    "        f.write(\"#endif // Q07_WEIGHTS_H\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = quantize_tensor_q07(net.fc1.weight)\n",
    "b = quantize_tensor_q07(net.fc1.bias)\n",
    "q2 = quantize_tensor_q07(net.fc2.weight)\n",
    "b2 = quantize_tensor_q07(net.fc2.bias)\n",
    "np.savetxt(\"weights_fc1_q07.txt\", q)\n",
    "np.savetxt(\"bias_fc1_q07.txt\", b)\n",
    "np.savetxt(\"weights_fc2_q07.txt\", q2)\n",
    "np.savetxt(\"bias_fc2_q07.txt\", b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_c_header(net, \"q07_weights.h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0\n",
      "Train Set Loss: 2.30\n",
      "Test Set Loss: 2.28\n",
      "Train set accuracy for a single minibatch: 34.00%\n",
      "Test set accuracy for a single minibatch: 23.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 50\n",
      "Train Set Loss: 0.94\n",
      "Test Set Loss: 0.80\n",
      "Train set accuracy for a single minibatch: 76.00%\n",
      "Test set accuracy for a single minibatch: 82.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 100\n",
      "Train Set Loss: 0.55\n",
      "Test Set Loss: 0.37\n",
      "Train set accuracy for a single minibatch: 85.00%\n",
      "Test set accuracy for a single minibatch: 93.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 150\n",
      "Train Set Loss: 0.47\n",
      "Test Set Loss: 0.31\n",
      "Train set accuracy for a single minibatch: 87.00%\n",
      "Test set accuracy for a single minibatch: 93.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 200\n",
      "Train Set Loss: 0.38\n",
      "Test Set Loss: 0.29\n",
      "Train set accuracy for a single minibatch: 85.00%\n",
      "Test set accuracy for a single minibatch: 92.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 250\n",
      "Train Set Loss: 0.23\n",
      "Test Set Loss: 0.32\n",
      "Train set accuracy for a single minibatch: 95.00%\n",
      "Test set accuracy for a single minibatch: 92.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 300\n",
      "Train Set Loss: 0.36\n",
      "Test Set Loss: 0.29\n",
      "Train set accuracy for a single minibatch: 91.00%\n",
      "Test set accuracy for a single minibatch: 91.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 350\n",
      "Train Set Loss: 0.33\n",
      "Test Set Loss: 0.24\n",
      "Train set accuracy for a single minibatch: 89.00%\n",
      "Test set accuracy for a single minibatch: 93.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 400\n",
      "Train Set Loss: 0.40\n",
      "Test Set Loss: 0.31\n",
      "Train set accuracy for a single minibatch: 89.00%\n",
      "Test set accuracy for a single minibatch: 91.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 450\n",
      "Train Set Loss: 0.29\n",
      "Test Set Loss: 0.18\n",
      "Train set accuracy for a single minibatch: 90.00%\n",
      "Test set accuracy for a single minibatch: 96.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 500\n",
      "Train Set Loss: 0.15\n",
      "Test Set Loss: 0.18\n",
      "Train set accuracy for a single minibatch: 95.00%\n",
      "Test set accuracy for a single minibatch: 95.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 550\n",
      "Train Set Loss: 0.26\n",
      "Test Set Loss: 0.29\n",
      "Train set accuracy for a single minibatch: 95.00%\n",
      "Test set accuracy for a single minibatch: 93.00%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.relu(self.fc1(x))\n",
    "        fc2_out = self.fc2(x)\n",
    "        return fc2_out\n",
    "\n",
    "ann = ANN().to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ann.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output = ann(data.view(batch_size, -1))\n",
    "    _, idx = output.max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, train=False)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        ann.train()\n",
    "        out = ann(data.view(batch_size, -1))\n",
    "\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        loss_val += loss(out, targets)\n",
    "\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad():\n",
    "            ann.eval()\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            out = ann(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "            test_loss += loss(out, test_targets)\n",
    "\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            if counter % 50 == 0:\n",
    "                train_printer()\n",
    "            counter += 1\n",
    "            iter_counter +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correctly classified test set images: 9354/10000\n",
      "Test Set Accuracy: 93.54%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# drop_last switched to False to keep all samples\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "  ann.eval()\n",
    "  for data, targets in test_loader:\n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    out = ann(data.view(data.size(0), -1))\n",
    "\n",
    "    # calculate total accuracy\n",
    "    _, predicted = out.max(1)\n",
    "    total += targets.size(0)\n",
    "    correct += (predicted == targets).sum().item()\n",
    "print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
    "print(f\"Test Set Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\Users\\marca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\marca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMyNetTF\u001b[39;00m(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\marca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mc:\\Users\\marca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\Users\\marca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MyNetTF(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation=\"relu\", input_shape=(784,))\n",
    "        self.fc2 = tf.keras.layers.Dense(10)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.reshape(x, (-1, 784))\n",
    "        x = self.fc1(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Instantiate and build once (so weights are created)\n",
    "model_tf = MyNetTF()\n",
    "_ = model_tf(tf.zeros([1, 784]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
