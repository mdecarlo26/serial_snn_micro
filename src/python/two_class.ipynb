{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from snntorch import spikegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"../data.txt\")          # shape (400, 2)\n",
    "labels = np.loadtxt(\"../labels.txt\")      # shape (400,)\n",
    "labels = labels.astype(int)\n",
    "\n",
    "# === Convert to PyTorch tensors ===\n",
    "X = torch.tensor(data, dtype=torch.float32)\n",
    "y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "num_steps = 20\n",
    "# === Create DataLoader ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        beta = 0.9  # Membrane decay rate\n",
    "        self.fc1 = nn.Linear(2, 16)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "\n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        spk2_rec = []\n",
    "\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        spike_input = spikegen.rate(x, num_steps=num_steps)\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(spike_input[step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            spk2_rec.append(spk2)\n",
    "\n",
    "        # Sum spikes over time across timesteps\n",
    "        return torch.stack(spk2_rec, dim=0).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SNN()\n",
    "loss_fn = SF.ce_count_loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch (got input: [2], target: [32])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m out \u001b[38;5;241m=\u001b[39m net(batch_x)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Loss: expects raw spike counts and class indices\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\marca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\snntorch\\functional\\loss.py:21\u001b[0m, in \u001b[0;36mLossFunctions.__call__\u001b[1;34m(self, spk_out, targets)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, spk_out, targets):\n\u001b[1;32m---> 21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspk_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce(loss)\n",
      "File \u001b[1;32mc:\\Users\\marca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\snntorch\\functional\\loss.py:179\u001b[0m, in \u001b[0;36mce_count_loss._compute_loss\u001b[1;34m(self, spk_out, targets)\u001b[0m\n\u001b[0;32m    176\u001b[0m     spike_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(spk_out, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# B x C\u001b[39;00m\n\u001b[0;32m    177\u001b[0m log_p_y \u001b[38;5;241m=\u001b[39m log_softmax_fn(spike_count)\n\u001b[1;32m--> 179\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_p_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\marca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\marca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:251\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3158\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   3156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3157\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3159\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\n\u001b[0;32m   3160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch (got input: [2], target: [32])"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        out = net(batch_x)\n",
    "\n",
    "        # Loss: expects raw spike counts and class indices\n",
    "        loss = loss_fn(out, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marca\\AppData\\Local\\Temp\\ipykernel_20684\\3416006048.py:14: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n",
      "    * make sure the original data is stored as integers.\n",
      "    * use the `converters=` keyword argument.  If you only use\n",
      "      NumPy 1.23 or later, `converters=float` will normally work.\n",
      "    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n",
      "      floating point and then convert it.  (On all NumPy versions.)\n",
      "  (Deprecated NumPy 1.23)\n",
      "  labels = np.loadtxt(\"../labels.txt\", dtype=np.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Average Loss: 14.1056\n",
      "Epoch [2/20], Average Loss: 14.1027\n",
      "Epoch [3/20], Average Loss: 14.1128\n",
      "Epoch [4/20], Average Loss: 14.0549\n",
      "Epoch [5/20], Average Loss: 14.0444\n",
      "Epoch [6/20], Average Loss: 14.0294\n",
      "Epoch [7/20], Average Loss: 14.0265\n",
      "Epoch [8/20], Average Loss: 13.9826\n",
      "Epoch [9/20], Average Loss: 13.8664\n",
      "Epoch [10/20], Average Loss: 13.9056\n",
      "Epoch [11/20], Average Loss: 13.8852\n",
      "Epoch [12/20], Average Loss: 13.8752\n",
      "Epoch [13/20], Average Loss: 13.8700\n",
      "Epoch [14/20], Average Loss: 13.7787\n",
      "Epoch [15/20], Average Loss: 13.7929\n",
      "Epoch [16/20], Average Loss: 13.7318\n",
      "Epoch [17/20], Average Loss: 13.7233\n",
      "Epoch [18/20], Average Loss: 13.6839\n",
      "Epoch [19/20], Average Loss: 13.5925\n",
      "Epoch [20/20], Average Loss: 13.5273\n",
      "Spike data and labels saved as CSV files.\n"
     ]
    }
   ],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import spikegen\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 32\n",
    "num_steps = 20\n",
    "beta = 0.95\n",
    "\n",
    "# Load 2D dataset\n",
    "data = np.loadtxt(\"../data.txt\", dtype=np.float32)\n",
    "labels = np.loadtxt(\"../labels.txt\", dtype=np.int64)\n",
    "X = torch.tensor(data)\n",
    "y = torch.tensor(labels)\n",
    "\n",
    "# Create train/test split\n",
    "dataset = TensorDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Define network adapted to 2D input\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 16)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        spike_input = spikegen.rate(x, num_steps=num_steps)\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        spk_rec, mem_rec = [], []\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(spike_input[step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            spk_rec.append(spk2)\n",
    "            mem_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk_rec), torch.stack(mem_rec)\n",
    "\n",
    "net = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        _, mem_rec = net(inputs)\n",
    "\n",
    "        loss_val = sum(criterion(mem_rec[step], targets) for step in range(num_steps))\n",
    "\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss_val.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save spike data and labels from test set\n",
    "spike_save_path = \"input_spikes.csv\"\n",
    "label_save_path = \"labels.csv\"\n",
    "\n",
    "all_spikes = []\n",
    "all_labels = []\n",
    "\n",
    "for data, targets in test_loader:\n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    spike_data = spikegen.rate(data, num_steps=num_steps).cpu().numpy()\n",
    "    spike_data = np.squeeze(spike_data, axis=1)\n",
    "    all_spikes.append(spike_data)\n",
    "\n",
    "    all_labels.append(targets.cpu().numpy())\n",
    "\n",
    "all_spikes = np.concatenate(all_spikes, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "np.savetxt(spike_save_path, all_spikes.astype(np.int8), delimiter=\",\", fmt=\"%d\")\n",
    "np.savetxt(label_save_path, all_labels.astype(np.int8), delimiter=\",\", fmt=\"%d\")\n",
    "\n",
    "print(\"Spike data and labels saved as CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"2d_weights_fc1.txt\", net.fc1.weight.detach().numpy())\n",
    "np.savetxt(\"2d_weights_fc2.txt\", net.fc2.weight.detach().numpy())\n",
    "np.savetxt(\"2d_bias_fc1.txt\", net.fc1.bias.detach().numpy())\n",
    "np.savetxt(\"2d_bias_fc2.txt\", net.fc2.bias.detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
